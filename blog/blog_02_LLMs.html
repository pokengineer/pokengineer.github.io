<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-6NBJLF4H5J"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-6NBJLF4H5J');
    </script>

    <meta charset="utf-8">
    <title>LLMs</title>
    <link rel="icon" href="https://raw.githubusercontent.com/pokengineer/pokengineer.github.io/main/assets/imgs/icon.ico">

    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="author" content="Pokengineer">
    <!-- font icons -->
    <link rel="stylesheet" href="../assets/vendors/themify-icons/css/themify-icons.css">
    <!-- Bootstrap + LeadMark main styles -->
	<link rel="stylesheet" href="../assets/css/leadmark.css">

    <style>
        .hideextra { white-space: nowrap; overflow: hidden; text-overflow:ellipsis; }
    </style>
</head>
<body data-spy="scroll" data-target=".navbar" data-offset="40" id="home">

    <!-- page Navigation -->
    <nav class="navbar custom-navbar navbar-expand-md navbar-light fixed-top" data-spy="affix" data-offset-top="10">
        <div class="container">
            <a class="navbar-brand" href="../blog.html">
                <img src="https://raw.githubusercontent.com/pokengineer/pokengineer.github.io/main/assets/imgs/icon.ico" alt="">
            </a>
            <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav ml-auto">                    
                    <li class="nav-item">
                        <a href="../blog.html" class="ml-4 nav-link btn btn-primary btn-sm rounded">Blog</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <!-- End Of Second Navigation -->


    <!-- About Section -->
    <section class="section" id="about">
        <div class="container">
            <div class="row justify-content-between">
                <div class="col-md-6 pr-md-5 mb-4 mb-md-0">
                </div>
            </div>              
        </div>
    </section>
    <!-- End OF About Section -->

    <!-- Service Section -->
    <section  id="latest" class="section mb-0">
        <div class="container">
            <h1>Hoy un amigo te explica LLMs</h1>
            <p>LLM es un “modelo de lenguaje grande” (por sus siglas en inglés), un tipo de programa de inteligencia artificial (IA) que puede procesar y generar texto, entre otras tareas relacionadas con el lenguaje natural.</p>
            <p>ChatGPT y otros modelos generativos pre-entrenados son los primeros que vienen a la mente cuando pensamos en LLMs. Y aunque mucha gente interactúa con estos modelos a diario, pocos comprenden realmente cómo funcionan. El objetivo de este post es que todos podamos entender sus principios básicos.</p>
            
            <h2>Enfoque Probabilístico</h2>
            <p>
                Hasta hace unos pocos años el procesamiento de lenguaje natural (usar computadoras para tareas relacionadas al lenguaje) era una tarea que se apoyaba 100% en el uso de la estadística. Usando información estadística de un idioma en particular, se podía crear un algoritmo como el modelo de <b>n-gramas</b> , que predice la siguiente palabra en una secuencia en base a las anteriores.
            </p>
            <p>Si bien estos modelos eran simples y eficaces en tareas básicas como la corrección ortográfica o el reconocimiento de voz, su capacidad de comprensión era muy limitada. (es por definición una cadena de Markov de orden n-1, pero los detalles de cómo se calcula exceden este post).</p>

            <img src="../assets/imgs/blog_02_llm1.jpg" alt="stuff-1" style="max-width: 100%; max-height: 60%;" >
            
            <h2>Word embeddings</h2>
            <p>Cerca del 2010, se empieza a utilizar una herramienta muy útil para el procesamiento de lenguaje natural que consiste básicamente en tratar de representar palabras y/o frases como “vectores numéricos”. Una de las técnicas más reconocidas de Word-Embedding es Word2Vec, desarrollada por un equipo de google en el 2013, que representa cualquier palabra del idioma inglés en un vector de 300 dimensiones y luego nos permite calcular una “distancia” entre estos vectores para saber qué tan similares son dos palabras entre sí.</p>
            <p>Por ejemplo, las palabras “perro”, “cachorro” y “can” son muy parecidas en su significado (prácticamente sinónimos), mientras que una palabra como “gato” no es un sinónimo pero podría aparecer en la misma posición en una oración (no está tan “lejos”) y la palabra “viajar” seguramente esté muy lejos en ese espacio vectorial.</p>
            <p>Este cambio permite que un modelo de lenguaje entienda palabras similares en significado como equivalentes, mejorando exponencialmente el potencial de los modelos, de repente “pájaro” y “volar” son reconocidas por estos modelos matemáticos como parecidas no por su uso en una oración, sino por su significado.</p>

            <h2>GPTs</h2>
            <p>Desde el 2017, el tipo de modelo más utilizado se conoce como “transformador”, son modelos de aprendizaje profundo que mediante el uso de redes neuronales recurrentes ponderan cuánta “atención” prestarle a cada palabra de un texto, con esto revolucionando el campo de procesamiento de lenguaje.</p>
            <p>Su clave está en el mecanismo de <b>atención</b>, que permite al modelo enfocarse en las partes más relevantes del texto, sin importar la distancia entre palabras. Esta arquitectura dio lugar a los <b>transformers generativos pre-entrenados (GPTs)</b>, donde podemos partir de una red neuronal ya entrenada para darle un uso general o refinar a un uso específico requerido.</p>
            <p>En el 2022 OpenAi lanza al público chatGPT, un chatbot entrenado con una enorme cantidad de información (por eso les dicen modelos de lenguajes grandes) que permite generar texto siguiendo una instrucción (o prompt) escrita también en lenguaje natural.</p>
            
            <h2>RAGs</h2>
            <p>Al momento de escribir esto la última mejora a estos modelos es el concepto de RAG. Antes se hablaba de dos tipos de modelos de lenguajes con funcionalidades muy distintas, modelos de recuperación y modelos generativos.</p>
            <p>Un ejemplo de <b>modelo generativo</b> es chatGPT 3.0, que con una instrucción simple era capaz de generar texto con todo lo que mencionamos hasta ahora, entrenado en una gran cantidad de textos de ejemplo. Por otro lado un <b>modelo de recuperación</b> tiene una funcionalidad más específica, y es encontrar en una base de datos alguna información que solicitamos, similar a lo que hacía la encarta de Microsoft para traer resultados relevantes a lo que escribimos, pero haciendo una búsqueda más compleja, semántica y contextualizada.</p>
            <p>El modelo RAG (Retrieval-Augmented Generation) combina ambas estrategias, y nos permite generar texto usando un modelo pre-entrenado, agregando en tiempo real fuentes nuevas.</p>

            <h2>Conclusión</h2>
            <p>En resumen, los modelos de lenguaje han pasado de simples predicciones estadísticas a sistemas sofisticados capaces de entender, generar y razonar con texto. Con avances como RAG, el futuro del lenguaje automatizado parece cada vez más integrado a nuestras vidas cotidianas, desde búsquedas hasta asistentes inteligentes.</p>
        </div>
    </section>
    <!-- End OF Service Section -->


    <!-- Contact Section 
    <section id="end" class="section has-img-bg pb-0">
        <div class="container">
            <footer class="mt-5 py-4 border-secondary">
                   
            </footer>
        </div>
    </section> 
    -->
	
	<!-- core  -->
    <script src="../assets/vendors/jquery/jquery-3.4.1.js"></script>
    <script src="../assets/vendors/bootstrap/bootstrap.bundle.js"></script>

    <!-- bootstrap 3 affix -->
	<script src="../assets/vendors/bootstrap/bootstrap.affix.js"></script>

    <!-- Isotope -->
    <script src="../assets/vendors/isotope/isotope.pkgd.js"></script>

    <!-- LeadMark js -->
    <script src="../assets/js/leadmark.js"></script>

</body>
</html>
